{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import torch.profiler as profiler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from copy import deepcopy\n"
      ],
      "metadata": {
        "id": "iMA8fxIvK2vr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "ou1gmCrKK5zM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import copy\n",
        "import numpy as np\n",
        "from thop import profile\n",
        "\n",
        "\n",
        "############################################\n",
        "# Device Selection\n",
        "############################################\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "############################################\n",
        "# MNIST loading\n",
        "############################################\n",
        "# Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "############################################\n",
        "# Model Definition\n",
        "############################################\n",
        "# Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 28 * 28, 10)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
        "        nn.init.constant_(self.conv1.bias, 0)\n",
        "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
        "        nn.init.constant_(self.conv2.bias, 0)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.constant_(self.fc1.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "def train_one_epoch(model, optimizer, criterion, dataloader, device='cpu'):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for (images, labels) in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    return running_loss / total, 100.0 * correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device='cpu'):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for (images, labels) in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return running_loss / total, 100.0 * correct / total\n",
        "\n",
        "def prune_conv_layer(conv_layer, prune_ratio=0.5, device='cpu'):\n",
        "    w = conv_layer.weight.data\n",
        "    out_channels = w.shape[0]\n",
        "\n",
        "    # Compute L1-norm\n",
        "    channel_norms = w.abs().mean(dim=(1,2,3))\n",
        "    keep_channels = int(out_channels * (1 - prune_ratio))\n",
        "    _, indices_to_keep = torch.topk(channel_norms, k=keep_channels)\n",
        "    indices_to_keep = indices_to_keep.sort()[0]\n",
        "\n",
        "    new_w = w[indices_to_keep, :, :, :].clone().to(device)\n",
        "    if conv_layer.bias is not None:\n",
        "        new_bias = conv_layer.bias.data[indices_to_keep].clone().to(device)\n",
        "    else:\n",
        "        new_bias = None\n",
        "\n",
        "    new_conv = nn.Conv2d(\n",
        "        in_channels=conv_layer.in_channels,\n",
        "        out_channels=keep_channels,\n",
        "        kernel_size=conv_layer.kernel_size,\n",
        "        stride=conv_layer.stride,\n",
        "        padding=conv_layer.padding,\n",
        "        dilation=conv_layer.dilation,\n",
        "        groups=conv_layer.groups,\n",
        "        bias=(conv_layer.bias is not None)\n",
        "    ).to(device)\n",
        "\n",
        "    # Re-init weights for stability\n",
        "    nn.init.kaiming_uniform_(new_conv.weight, nonlinearity='relu')\n",
        "    nn.init.constant_(new_conv.bias, 0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        new_conv.weight.data = new_w\n",
        "        if new_bias is not None:\n",
        "            new_conv.bias.data = new_bias\n",
        "\n",
        "    return new_conv\n",
        "\n",
        "# Create baseline model\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
        "flops_before, _ = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "\n",
        "##########################################\n",
        "# Train Baseline Model (5 epochs)\n",
        "##########################################\n",
        "print(\"=== Baseline Model Training ===\")\n",
        "if device == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "baseline_start = time.time()\n",
        "for epoch in range(5):\n",
        "    train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {val_loss:.4f}, Test Acc: {val_acc:.2f}%\")\n",
        "if device == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "baseline_end = time.time()\n",
        "baseline_total_time = baseline_end - baseline_start\n",
        "\n",
        "baseline_val_loss, baseline_val_acc = val_loss, val_acc\n",
        "if device == 'cuda':\n",
        "    baseline_memory = torch.cuda.max_memory_allocated(device)\n",
        "else:\n",
        "    baseline_memory = None\n",
        "\n",
        "##########################################\n",
        "# Prune both conv1 and conv2 to improve the model\n",
        "##########################################\n",
        "pruned_model = copy.deepcopy(model).to(device)\n",
        "pruned_model.eval()\n",
        "\n",
        "# Prune conv1 by 50%\n",
        "pruned_model.conv1 = prune_conv_layer(pruned_model.conv1, prune_ratio=0.5, device=device)\n",
        "\n",
        "# After pruning conv1, the input channels of conv2 must be adjusted\n",
        "# The original conv2 expects 16 channels in. After pruning conv1 by half, we have 8 channels out of conv1 now.\n",
        "# We must re-initialize conv2 accordingly.\n",
        "old_conv2 = pruned_model.conv2\n",
        "in_channels_new = pruned_model.conv1.out_channels\n",
        "out_channels_old = old_conv2.out_channels\n",
        "\n",
        "# Create a new conv2 with updated in_channels\n",
        "new_conv2 = nn.Conv2d(in_channels_new, out_channels_old, kernel_size=3, padding=1, bias=True).to(device)\n",
        "nn.init.kaiming_uniform_(new_conv2.weight, nonlinearity='relu')\n",
        "nn.init.constant_(new_conv2.bias, 0)\n",
        "pruned_model.conv2 = new_conv2\n",
        "\n",
        "# Now prune conv2 by 50%\n",
        "pruned_model.conv2 = prune_conv_layer(pruned_model.conv2, prune_ratio=0.5, device=device)\n",
        "\n",
        "# Adjust FC layer\n",
        "new_out_channels = pruned_model.conv2.out_channels\n",
        "new_fc_in_features = new_out_channels * 28 * 28\n",
        "pruned_model.fc1 = nn.Linear(new_fc_in_features, 10).to(device)\n",
        "nn.init.xavier_uniform_(pruned_model.fc1.weight)\n",
        "nn.init.constant_(pruned_model.fc1.bias, 0)\n",
        "\n",
        "optimizer_pruned = optim.Adam(pruned_model.parameters(), lr=1e-3)\n",
        "\n",
        "flops_after, _ = profile(pruned_model, inputs=(dummy_input,), verbose=False)\n",
        "\n",
        "#####################\n",
        "# Train Pruned Model (5 epochs)\n",
        "#####################\n",
        "print(\"=== Pruned Model Training ===\")\n",
        "if device == 'cuda':\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "pruned_start = time.time()\n",
        "for epoch in range(5):\n",
        "    train_loss_p, train_acc_p = train_one_epoch(pruned_model, optimizer_pruned, criterion, train_loader, device)\n",
        "    val_loss_p, val_acc_p = evaluate(pruned_model, test_loader, criterion, device)\n",
        "    print(f\"[Pruned] Epoch {epoch+1}: Train Loss: {train_loss_p:.4f}, Train Acc: {train_acc_p:.2f}%, Test Loss: {val_loss_p:.4f}, Test Acc: {val_acc_p:.2f}%\")\n",
        "if device == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "pruned_end = time.time()\n",
        "pruned_total_time = pruned_end - pruned_start\n",
        "\n",
        "pruned_val_loss, pruned_val_acc = val_loss_p, val_acc_p\n",
        "if device == 'cuda':\n",
        "    pruned_memory = torch.cuda.max_memory_allocated(device)\n",
        "else:\n",
        "    pruned_memory = None\n",
        "\n",
        "#####################\n",
        "# Final Report\n",
        "#####################\n",
        "print(\"\\n=== Final Results Summary ===\")\n",
        "print(\"Baseline Model Final Metrics:\")\n",
        "print(f\" - Final Test Loss: {baseline_val_loss:.4f}\")\n",
        "print(f\" - Final Test Acc: {baseline_val_acc:.2f}%\")\n",
        "print(f\" - FLOPs Before Pruning: {flops_before}\")\n",
        "print(f\" - Total Training Time (5 epochs): {baseline_total_time:.2f}s\")\n",
        "if baseline_memory is not None:\n",
        "    print(f\" - Peak Memory Usage: {baseline_memory / (1024**2):.2f} MB\")\n",
        "\n",
        "print(\"\\nPruned Model Final Metrics:\")\n",
        "print(f\" - Final Test Loss: {pruned_val_loss:.4f}\")\n",
        "print(f\" - Final Test Acc: {pruned_val_acc:.2f}%\")\n",
        "print(f\" - FLOPs After Pruning: {flops_after}\")\n",
        "print(f\" - Total Training Time (5 epochs): {pruned_total_time:.2f}s\")\n",
        "if pruned_memory is not None:\n",
        "    print(f\" - Peak Memory Usage: {pruned_memory / (1024**2):.2f} MB\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"FLOPs reduction: {flops_before} -> {flops_after}\")\n",
        "print(f\"Time Reduction (training): {baseline_total_time:.2f}s -> {pruned_total_time:.2f}s\")\n",
        "if baseline_memory is not None and pruned_memory is not None:\n",
        "    print(f\"Memory Reduction: {baseline_memory / (1024**2):.2f} MB -> {pruned_memory / (1024**2):.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkHM_H1Knd8b",
        "outputId": "52e6cdb8-f2c1-4acb-f4b4-9230130d49ac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Baseline Model Training ===\n",
            "Epoch 1: Train Loss: 0.1866, Train Acc: 95.29%, Test Loss: 0.0773, Test Acc: 97.74%\n",
            "Epoch 2: Train Loss: 0.0424, Train Acc: 98.70%, Test Loss: 0.0549, Test Acc: 98.36%\n",
            "Epoch 3: Train Loss: 0.0222, Train Acc: 99.28%, Test Loss: 0.0576, Test Acc: 98.21%\n",
            "Epoch 4: Train Loss: 0.0149, Train Acc: 99.52%, Test Loss: 0.0616, Test Acc: 98.32%\n",
            "Epoch 5: Train Loss: 0.0125, Train Acc: 99.55%, Test Loss: 0.0664, Test Acc: 98.29%\n",
            "=== Pruned Model Training ===\n",
            "[Pruned] Epoch 1: Train Loss: 0.1385, Train Acc: 95.72%, Test Loss: 0.0722, Test Acc: 97.73%\n",
            "[Pruned] Epoch 2: Train Loss: 0.0447, Train Acc: 98.61%, Test Loss: 0.0570, Test Acc: 98.22%\n",
            "[Pruned] Epoch 3: Train Loss: 0.0234, Train Acc: 99.25%, Test Loss: 0.0598, Test Acc: 98.23%\n",
            "[Pruned] Epoch 4: Train Loss: 0.0162, Train Acc: 99.46%, Test Loss: 0.0653, Test Acc: 98.13%\n",
            "[Pruned] Epoch 5: Train Loss: 0.0106, Train Acc: 99.67%, Test Loss: 0.0652, Test Acc: 98.29%\n",
            "\n",
            "=== Final Results Summary ===\n",
            "Baseline Model Final Metrics:\n",
            " - Final Test Loss: 0.0664\n",
            " - Final Test Acc: 98.29%\n",
            " - FLOPs Before Pruning: 3976448.0\n",
            " - Total Training Time (5 epochs): 79.22s\n",
            " - Peak Memory Usage: 13289.67 MB\n",
            "\n",
            "Pruned Model Final Metrics:\n",
            " - Final Test Loss: 0.0652\n",
            " - Final Test Acc: 98.29%\n",
            " - FLOPs After Pruning: 1085056.0\n",
            " - Total Training Time (5 epochs): 78.73s\n",
            " - Peak Memory Usage: 13193.53 MB\n",
            "\n",
            "Comparison:\n",
            "FLOPs reduction: 3976448.0 -> 1085056.0\n",
            "Time Reduction (training): 79.22s -> 78.73s\n",
            "Memory Reduction: 13289.67 MB -> 13193.53 MB\n"
          ]
        }
      ]
    }
  ]
}